{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5pVKUep9pPnL"
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maJSw9gT37wN"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pi6hlCoX36MV"
   },
   "outputs": [],
   "source": [
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def calc_accuracy(model):\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        imgs, true_labels = data[0].to(device), data[1].to(device)\n",
    "        preds = model(imgs)\n",
    "        pred_class = torch.argmax(preds, dim=1)\n",
    "        incorrect = torch.count_nonzero(pred_class - true_labels)\n",
    "        correct += len(true_labels) - incorrect\n",
    "\n",
    "    print(f'test accuracy: {(correct * 100) / len(test):.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_SA-5kvypPnQ"
   },
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.MNIST('./datasets', train=True, download=True,\n",
    "                                   transform=torchvision.transforms.Compose([\n",
    "                                       torchvision.transforms.ToTensor(),\n",
    "                                       torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                   ]))\n",
    "\n",
    "train, test = torch.utils.data.random_split(mnist, [50000, 10000])\n",
    "batch_size = 256\n",
    "train_dataset = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "small_mnist_subset = list(range(0, int(len(mnist)*0.03)))\n",
    "small_train_dataset = torch.utils.data.DataLoader(torch.utils.data.Subset(mnist, small_mnist_subset), batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g45KtP7B3_T9"
   },
   "source": [
    "# Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHArzIzppPnR",
    "outputId": "ef3be6b1-1b94-4128-b7a5-8ff35f5a8373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 26, 26]             320\n",
      "              ReLU-2           [-1, 32, 26, 26]               0\n",
      "            Conv2d-3           [-1, 64, 24, 24]          18,496\n",
      "              ReLU-4           [-1, 64, 24, 24]               0\n",
      "         MaxPool2d-5           [-1, 64, 12, 12]               0\n",
      "         Dropout2d-6           [-1, 64, 12, 12]               0\n",
      "           Flatten-7                 [-1, 9216]               0\n",
      "            Linear-8                  [-1, 128]       1,179,776\n",
      "              ReLU-9                  [-1, 128]               0\n",
      "          Dropout-10                  [-1, 128]               0\n",
      "           Linear-11                   [-1, 10]           1,290\n",
      "          Softmax-12                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.11\n",
      "Params size (MB): 4.58\n",
      "Estimated Total Size (MB): 5.69\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Teacher model\n",
    "teacher_model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=(3, 3)),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    nn.Conv2d(32, 64, kernel_size=(3, 3)),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    nn.Conv2d(64, 128, kernel_size=(2, 2)),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2048, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(128, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "teacher_model.to(device)\n",
    "print(summary(teacher_model, (1, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhDSwAxGpPnS",
    "outputId": "3b640c95-f111-4173-b4a2-0f9d33e7a1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.7583, time: 10.49s\n",
      "epoch: 2, loss: 1.5801, time: 10.93s\n",
      "epoch: 3, loss: 1.5544, time: 11.07s\n",
      "epoch: 4, loss: 1.5379, time: 10.62s\n",
      "epoch: 5, loss: 1.5270, time: 10.50s\n",
      "test accuracy: 94.460%\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(teacher_model.parameters(), lr=0.0001)\n",
    "teacher_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(epochs):\n",
    "    start_time = time_s()\n",
    "    ep_loss = 0.\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        imgs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = teacher_model(imgs.to(device))\n",
    "        loss = teacher_loss_fn(preds, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ep_loss += loss.detach().item()\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(train_dataset):.4f}, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "calc_accuracy(teacher_model)\n",
    "\n",
    "torch.save(teacher_model.state_dict(), './models/teacher.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjZXzbsipPnT",
    "outputId": "58a9b08e-ba51-43d8-a36e-46ff32d7b9cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load teacher model after training\n",
    "teacher_model.load_state_dict(torch.load('./models/teacher.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzmcdWoi4Jhy"
   },
   "source": [
    "# Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L3J77XTrpPnT"
   },
   "outputs": [],
   "source": [
    "# Softmax with temperature\n",
    "# -- Adapted from PyTorch Softmax layer\n",
    "# -- See: https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softmax\n",
    "class SoftmaxT(nn.Module):\n",
    "    def __init__(self, temperature, dim = 1) -> None:\n",
    "        super(SoftmaxT, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dim = dim\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        if not hasattr(self, 'dim'):\n",
    "            self.dim = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.nn.functional.softmax(input / self.temperature, self.dim, _stacklevel=5)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'dim={dim}'.format(dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3czr7e1PpPnU",
    "outputId": "87135e7b-3501-48eb-cc16-b6bd188ac545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 26, 26]             320\n",
      "              ReLU-2           [-1, 32, 26, 26]               0\n",
      "            Conv2d-3           [-1, 64, 24, 24]          18,496\n",
      "              ReLU-4           [-1, 64, 24, 24]               0\n",
      "         MaxPool2d-5           [-1, 64, 12, 12]               0\n",
      "         Dropout2d-6           [-1, 64, 12, 12]               0\n",
      "           Flatten-7                 [-1, 9216]               0\n",
      "            Linear-8                  [-1, 128]       1,179,776\n",
      "              ReLU-9                  [-1, 128]               0\n",
      "          Dropout-10                  [-1, 128]               0\n",
      "           Linear-11                   [-1, 10]           1,290\n",
      "         SoftmaxT-12                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.11\n",
      "Params size (MB): 4.58\n",
      "Estimated Total Size (MB): 5.69\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 26, 26]             160\n",
      "              ReLU-2           [-1, 16, 26, 26]               0\n",
      "         MaxPool2d-3           [-1, 16, 13, 13]               0\n",
      "            Conv2d-4           [-1, 32, 11, 11]           4,640\n",
      "           Flatten-5                 [-1, 3872]               0\n",
      "            Linear-6                   [-1, 10]          38,730\n",
      "          SoftmaxT-7                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 43,530\n",
      "Trainable params: 43,530\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.24\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.41\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "temperature = 5\n",
    "\n",
    "# Create a new model with the last layer removed, provides access to model logits\n",
    "teacher_model_w_temperature = torch.nn.Sequential(\n",
    "    *(list(teacher_model.children())[:-1]),\n",
    "    SoftmaxT(temperature)\n",
    ")\n",
    "teacher_model_w_temperature.to(device)\n",
    "print(summary(teacher_model_w_temperature, (1, 28, 28)))\n",
    "\n",
    "# Create the student model\n",
    "student_model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=(3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    nn.Conv2d(16, 32, kernel_size=(3, 3)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3872, 10),\n",
    "    SoftmaxT(temperature)\n",
    ")\n",
    "student_model.to(device)\n",
    "print(summary(student_model, (1, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sNzVO_up7TkU",
    "outputId": "77513e87-92b8-46a5-81e6-842a235c9cf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 5.22e-03, time: 9.17s\n",
      "epoch: 2, loss: 1.24e-03, time: 9.21s\n",
      "epoch: 3, loss: 1.58e-04, time: 9.32s\n",
      "test accuracy: 92.000%\n"
     ]
    }
   ],
   "source": [
    "# KD from teacher using whole dataset\n",
    "student_model.apply(weight_reset)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=0.0001)\n",
    "student_loss_fn = nn.CrossEntropyLoss()\n",
    "distillation_loss_fn = torch.nn.KLDivLoss()\n",
    "\n",
    "kd_epochs = 3\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(kd_epochs):\n",
    "    ep_loss = 0.\n",
    "    start_time = time_s()\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        imgs, true_labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        student_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass of the teacher with input\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model_w_temperature(imgs).to(device)\n",
    "\n",
    "        # Forward pass of the student\n",
    "        student_output = student_model(imgs).to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        student_loss = student_loss_fn(student_output, true_labels)\n",
    "        distill_loss = distillation_loss_fn(teacher_output, student_output)\n",
    "        loss = alpha * student_loss + (1 - alpha) * distill_loss\n",
    "\n",
    "        loss.backward()\n",
    "        student_optimizer.step()\n",
    "        \n",
    "        ep_loss += loss.detach().item()\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(train_dataset):.2e}, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "calc_accuracy(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTkrLueJpPnU",
    "outputId": "0d56789a-568c-49a5-b381-e57b7db5f7e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.01e-02, time: 0.64s\n",
      "epoch: 2, loss: 3.70e-03, time: 0.64s\n",
      "epoch: 3, loss: 2.03e-03, time: 0.65s\n",
      "test accuracy: 85.300%\n"
     ]
    }
   ],
   "source": [
    "# KD from teacher using only ~3% of original dataset\n",
    "student_model.apply(weight_reset)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=0.0001)\n",
    "student_loss_fn = nn.CrossEntropyLoss()\n",
    "distillation_loss_fn = torch.nn.KLDivLoss()\n",
    "\n",
    "kd_epochs = 3\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(kd_epochs):\n",
    "    ep_loss = 0.\n",
    "    start_time = time_s()\n",
    "    for i, data in enumerate(small_train_dataset):\n",
    "        imgs, true_labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        student_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass of the teacher with input\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model_w_temperature(imgs).to(device)\n",
    "\n",
    "        # Forward pass of the student\n",
    "        student_output = student_model(imgs).to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        student_loss = student_loss_fn(student_output, true_labels)\n",
    "        distill_loss = distillation_loss_fn(teacher_output, student_output)\n",
    "        loss = alpha * student_loss + (1 - alpha) * distill_loss\n",
    "\n",
    "        loss.backward()\n",
    "        student_optimizer.step()\n",
    "        \n",
    "        ep_loss += loss.detach().item()\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(small_train_dataset):.2e}, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "calc_accuracy(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJlJYe-N0_CT"
   },
   "source": [
    "# Training student network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCWZ7xToZmKW",
    "outputId": "59058ec1-62b1-4a8e-c9e9-512e58c049cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 26, 26]             160\n",
      "              ReLU-2           [-1, 16, 26, 26]               0\n",
      "         MaxPool2d-3           [-1, 16, 13, 13]               0\n",
      "            Conv2d-4           [-1, 32, 11, 11]           4,640\n",
      "           Flatten-5                 [-1, 3872]               0\n",
      "            Linear-6                   [-1, 10]          38,730\n",
      "           Softmax-7                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 43,530\n",
      "Trainable params: 43,530\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.24\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.41\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create a new model with the last layer removed, provides access to model logits\n",
    "student_model_wo_temperature = torch.nn.Sequential(\n",
    "    *(list(student_model.children())[:-1]),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "student_model_wo_temperature.to(device)\n",
    "print(summary(student_model_wo_temperature, (1, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFf5zmeM1qn6",
    "outputId": "d83f43ec-446c-47c9-ffa9-dec477e3a9fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.5971, time: 8.09s\n",
      "epoch: 2, loss: 1.5595, time: 7.98s\n",
      "epoch: 3, loss: 1.5448, time: 7.93s\n",
      "test accuracy: 93.000%\n"
     ]
    }
   ],
   "source": [
    "# Using whole dataset\n",
    "epochs = 3\n",
    "student_wo_temp_optimizer = torch.optim.Adam(student_model_wo_temperature.parameters(), lr=0.0001)\n",
    "student_wo_temp_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(epochs):\n",
    "    start_time = time_s()\n",
    "    ep_loss = 0.\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        imgs, labels = data\n",
    "        \n",
    "        student_wo_temp_optimizer.zero_grad()\n",
    "        preds = student_model_wo_temperature(imgs.to(device))\n",
    "        loss = student_wo_temp_loss_fn(preds, labels.to(device))\n",
    "        loss.backward()\n",
    "        student_wo_temp_optimizer.step()\n",
    "        \n",
    "        ep_loss += loss.detach().item()\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(train_dataset):.4f}, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "calc_accuracy(student_model_wo_temperature)\n",
    "\n",
    "torch.save(student_model_wo_temperature.state_dict(), './models/student_wo_temp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8S6fShP2WAD",
    "outputId": "3b74f205-7a6f-44f3-9c7e-7e5e80803255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.0082, time: 0.55s\n",
      "epoch: 2, loss: 0.0068, time: 0.53s\n",
      "epoch: 3, loss: 0.0064, time: 0.53s\n",
      "test accuracy: 83.510%\n"
     ]
    }
   ],
   "source": [
    "# Using ~3% dataset\n",
    "# Note: Need to reinit the student_wo_temp model first!\n",
    "student_model_wo_temperature.apply(weight_reset)\n",
    "epochs = 3\n",
    "student_wo_temp_optimizer = torch.optim.Adam(student_model_wo_temperature.parameters(), lr=0.0001)\n",
    "student_wo_temp_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(epochs):\n",
    "    start_time = time_s()\n",
    "    ep_loss = 0.\n",
    "    for i, data in enumerate(small_train_dataset):\n",
    "        imgs, labels = data\n",
    "        \n",
    "        student_wo_temp_optimizer.zero_grad()\n",
    "        preds = student_model_wo_temperature(imgs.to(device))\n",
    "        loss = student_wo_temp_loss_fn(preds, labels.to(device))\n",
    "        loss.backward()\n",
    "        student_wo_temp_optimizer.step()\n",
    "        \n",
    "        ep_loss += loss.detach().item() / batch_size\n",
    "\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(small_train_dataset):.4f}, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "calc_accuracy(student_model_wo_temperature)\n",
    "\n",
    "torch.save(student_model_wo_temperature.state_dict(), './models/student_wo_temp.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}