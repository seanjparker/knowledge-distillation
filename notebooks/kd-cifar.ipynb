{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHeNYIJRpSdN",
    "outputId": "30096e23-9666-4623-abcf-ed008b052c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torchvision.models as models\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppgs4aCPpSdR",
    "outputId": "028eb594-5f92-4f70-991b-9810dbc2d859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "train_transforms = [\n",
    "    torchvision.transforms.Pad(4, padding_mode='reflect'),\n",
    "    torchvision.transforms.RandomCrop(32),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "]\n",
    "\n",
    "test_transforms = [\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "]\n",
    "\n",
    "cifar_train = torchvision.datasets.CIFAR10('./datasets', train=True, download=True,\n",
    "  transform=torchvision.transforms.Compose(train_transforms))\n",
    "\n",
    "cifar_test = torchvision.datasets.CIFAR10('./datasets', train=False, download=True,\n",
    "  transform=torchvision.transforms.Compose(test_transforms))\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = torch.utils.data.DataLoader(cifar_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_dataset = torch.utils.data.DataLoader(cifar_test, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "runoulM7pSdS"
   },
   "outputs": [],
   "source": [
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def calc_accuracy(model):\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataset):\n",
    "            imgs, true_labels = data[0].to(device), data[1].to(device)\n",
    "            preds = model(imgs)\n",
    "            pred_class = torch.argmax(preds, dim=1)\n",
    "            incorrect = torch.count_nonzero(pred_class - true_labels)\n",
    "            correct += len(true_labels) - incorrect\n",
    "\n",
    "    print(f'test accuracy: {(correct * 100) / len(test):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNEs71Ju9aN"
   },
   "source": [
    "# ResNet-18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RW9rIlspSdS"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "teacher_model = ResNet(BasicBlock, [2, 2, 2, 2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkpNnwPE8cVV",
    "outputId": "812c37d8-e428-49be-b0d0-94067639067f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 83.210%\n"
     ]
    }
   ],
   "source": [
    "teacher_model.load_state_dict(torch.load('./models/cifar_teacher_0.pt'))\n",
    "#print(summary(teacher_model, (3, 32, 32)))\n",
    "calc_accuracy(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7EFp_UUpSdS"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(teacher_model.parameters(), lr=0.01)\n",
    "teacher_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(epochs):\n",
    "    start_time = time_s()\n",
    "    ep_loss = 0.0\n",
    "    correct = 0\n",
    "    for i, (imgs, labels) in enumerate(train_dataset):\n",
    "        optimizer.zero_grad()\n",
    "        preds = teacher_model(imgs)\n",
    "        loss = teacher_loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred_class = torch.argmax(preds, dim=1).to(device)\n",
    "        correct += len(labels) - torch.count_nonzero(pred_class - labels)\n",
    "        \n",
    "        ep_loss += loss.detach().item()\n",
    "\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(train_dataset):.4f}, train acc: {(correct * 100.0) / len(cifar_train):.3f}%, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "calc_accuracy(teacher_model)\n",
    "\n",
    "torch.save(teacher_model.state_dict(), './models/cifar_teacher.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwlgvSkLHsOH"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxeJM6dSHqgP"
   },
   "outputs": [],
   "source": [
    "def kd_train(dataset, teacher, student, lr=0.001, student_optimizer = None, lr_scheduler = None):\n",
    "    student.apply(weight_reset)\n",
    "    student_optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
    "    student_lr_sch = torch.optim.lr_scheduler.MultiStepLR(student_optimizer, [10, 25, 40], gamma=0.5)\n",
    "    student_loss_fn = nn.CrossEntropyLoss()\n",
    "    distillation_loss_fn = torch.nn.KLDivLoss(reduction='mean')\n",
    "\n",
    "    kd_epochs = 30\n",
    "    time_s = lambda: time.time()\n",
    "    for ep in range(kd_epochs):\n",
    "        ep_loss = 0.0\n",
    "        correct = 0\n",
    "        start_time = time_s()\n",
    "        for i, (imgs, labels) in enumerate(train_dataset):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            student_optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass of the teacher with input\n",
    "            with torch.no_grad():\n",
    "                teacher_output = teacher(imgs).to(device)\n",
    "\n",
    "            # Forward pass of the student\n",
    "            student_output = student(imgs).to(device)\n",
    "\n",
    "            # Calculate loss\n",
    "            student_loss = student_loss_fn(student_output, labels)\n",
    "            distill_loss = distillation_loss_fn(teacher_output, student_output)\n",
    "            loss = (alpha * student_loss + (1 - alpha) * distill_loss) * temperature * temperature\n",
    "\n",
    "            loss.backward()\n",
    "            student_optimizer.step()\n",
    "\n",
    "            pred_class = torch.argmax(student_output, dim=1).to(device)\n",
    "            correct += len(labels) - torch.count_nonzero(pred_class - labels)\n",
    "\n",
    "            ep_loss += loss.detach().item()\n",
    "        student_lr_sch.step()\n",
    "        print(f'epoch: {ep+1}, loss: {ep_loss / len(train_dataset):.2e}, train acc: {(correct * 100.0) / len(cifar_train):.3f}%, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            calc_accuracy(student_model)\n",
    "    calc_accuracy(student_model)\n",
    "\n",
    "    torch.save(student_model.state_dict(), './models/cifar_student.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgfbU6JGpSdT"
   },
   "source": [
    "# Building Teacher and Student models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og0hg8AxpSdT"
   },
   "outputs": [],
   "source": [
    "# Softmax with temperature\n",
    "# -- Adapted from PyTorch Softmax layer\n",
    "# -- See: https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softmax\n",
    "class SoftmaxT(nn.Module):\n",
    "    def __init__(self, temperature, dim = 1) -> None:\n",
    "        super(SoftmaxT, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dim = dim\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        if not hasattr(self, 'dim'):\n",
    "            self.dim = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.nn.functional.softmax(input / self.temperature, self.dim)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'dim={dim}'.format(dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7Yfeq-GpSdT",
    "outputId": "06e8f82f-1f94-4b2a-c1ca-c0b33730f3fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 83.220%\n",
      "None\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 30, 30]             896\n",
      "       BatchNorm2d-2           [-1, 32, 30, 30]              64\n",
      "              ReLU-3           [-1, 32, 30, 30]               0\n",
      "         MaxPool2d-4           [-1, 32, 15, 15]               0\n",
      "            Conv2d-5           [-1, 64, 13, 13]          18,496\n",
      "       BatchNorm2d-6           [-1, 64, 13, 13]             128\n",
      "              ReLU-7           [-1, 64, 13, 13]               0\n",
      "         MaxPool2d-8             [-1, 64, 6, 6]               0\n",
      "           Flatten-9                 [-1, 2304]               0\n",
      "           Linear-10                  [-1, 128]         295,040\n",
      "             ReLU-11                  [-1, 128]               0\n",
      "          Dropout-12                  [-1, 128]               0\n",
      "           Linear-13                   [-1, 64]           8,256\n",
      "             ReLU-14                   [-1, 64]               0\n",
      "          Dropout-15                   [-1, 64]               0\n",
      "           Linear-16                   [-1, 10]             650\n",
      "         SoftmaxT-17                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 323,530\n",
      "Trainable params: 323,530\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 1.23\n",
      "Estimated Total Size (MB): 2.25\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "temperature = 4\n",
    "\n",
    "# Create a new model with softmax temperature\n",
    "teacher_model_w_temperature = torch.nn.Sequential(\n",
    "    teacher_model,\n",
    "    SoftmaxT(temperature)\n",
    ").to(device)\n",
    "#print(summary(teacher_model_w_temperature, (3, 32, 32)))\n",
    "print(calc_accuracy(teacher_model_w_temperature))\n",
    "\n",
    "# Create the student model\n",
    "student_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=(3, 3)),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    nn.Conv2d(32, 64, kernel_size=(3, 3)),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2304, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(64, 10),\n",
    "    SoftmaxT(temperature)\n",
    ").to(device)\n",
    "print(summary(student_model, (3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SEiKzQDpSdT",
    "outputId": "1a672447-f382-4f16-adde-1010b8434bef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.58e-01, train acc: 53.153%, time: 26.22s\n",
      "epoch: 2, loss: 1.12e-01, train acc: 67.860%, time: 26.54s\n",
      "epoch: 3, loss: 9.48e-02, train acc: 73.130%, time: 27.23s\n",
      "epoch: 4, loss: 8.47e-02, train acc: 75.842%, time: 26.85s\n",
      "epoch: 5, loss: 7.74e-02, train acc: 77.908%, time: 26.94s\n",
      "epoch: 6, loss: 7.17e-02, train acc: 79.855%, time: 27.37s\n",
      "epoch: 7, loss: 6.71e-02, train acc: 80.795%, time: 26.89s\n",
      "epoch: 8, loss: 6.30e-02, train acc: 81.985%, time: 26.73s\n",
      "epoch: 9, loss: 5.95e-02, train acc: 82.932%, time: 26.93s\n",
      "epoch: 10, loss: 5.59e-02, train acc: 83.812%, time: 26.72s\n",
      "test accuracy: 71.220%\n",
      "epoch: 11, loss: 4.69e-02, train acc: 85.825%, time: 26.97s\n",
      "epoch: 12, loss: 4.46e-02, train acc: 86.725%, time: 27.11s\n",
      "epoch: 13, loss: 4.36e-02, train acc: 86.950%, time: 27.01s\n",
      "epoch: 14, loss: 4.25e-02, train acc: 87.147%, time: 27.09s\n",
      "epoch: 15, loss: 4.09e-02, train acc: 87.592%, time: 27.04s\n",
      "epoch: 16, loss: 3.86e-02, train acc: 88.170%, time: 27.03s\n",
      "epoch: 17, loss: 3.85e-02, train acc: 88.300%, time: 27.21s\n",
      "epoch: 18, loss: 3.69e-02, train acc: 88.660%, time: 27.05s\n",
      "epoch: 19, loss: 3.60e-02, train acc: 88.912%, time: 27.03s\n",
      "epoch: 20, loss: 3.58e-02, train acc: 88.895%, time: 27.21s\n",
      "test accuracy: 74.280%\n",
      "epoch: 21, loss: 3.41e-02, train acc: 89.467%, time: 27.44s\n",
      "epoch: 22, loss: 3.35e-02, train acc: 89.680%, time: 26.97s\n",
      "epoch: 23, loss: 3.28e-02, train acc: 89.590%, time: 27.33s\n",
      "epoch: 24, loss: 3.14e-02, train acc: 90.290%, time: 27.12s\n",
      "epoch: 25, loss: 3.01e-02, train acc: 90.257%, time: 27.09s\n",
      "epoch: 26, loss: 2.71e-02, train acc: 91.020%, time: 27.06s\n",
      "epoch: 27, loss: 2.55e-02, train acc: 91.395%, time: 27.03s\n",
      "epoch: 28, loss: 2.57e-02, train acc: 91.587%, time: 26.86s\n",
      "epoch: 29, loss: 2.43e-02, train acc: 91.792%, time: 26.75s\n",
      "epoch: 30, loss: 2.42e-02, train acc: 91.675%, time: 26.69s\n",
      "test accuracy: 75.690%\n",
      "test accuracy: 75.880%\n"
     ]
    }
   ],
   "source": [
    "# KD from teacher using whole dataset\n",
    "student_model.apply(weight_reset)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)\n",
    "student_lr_sch = torch.optim.lr_scheduler.MultiStepLR(student_optimizer, [10, 25, 40], gamma=0.5)\n",
    "student_loss_fn = nn.CrossEntropyLoss()\n",
    "distillation_loss_fn = torch.nn.KLDivLoss(reduction='mean')\n",
    "\n",
    "kd_epochs = 30\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(kd_epochs):\n",
    "    ep_loss = 0.0\n",
    "    correct = 0\n",
    "    start_time = time_s()\n",
    "    for i, (imgs, labels) in enumerate(train_dataset):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        student_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass of the teacher with input\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model_w_temperature(imgs).to(device)\n",
    "\n",
    "        # Forward pass of the student\n",
    "        student_output = student_model(imgs).to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        student_loss = student_loss_fn(student_output, labels)\n",
    "        distill_loss = distillation_loss_fn(teacher_output, student_output)\n",
    "        loss = (alpha * student_loss + (1 - alpha) * distill_loss) * temperature * temperature\n",
    "\n",
    "        loss.backward()\n",
    "        student_optimizer.step()\n",
    "\n",
    "        pred_class = torch.argmax(student_output, dim=1).to(device)\n",
    "        correct += len(labels) - torch.count_nonzero(pred_class - labels)\n",
    "\n",
    "        ep_loss += loss.detach().item()\n",
    "    student_lr_sch.step()\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(train_dataset):.2e}, train acc: {(correct * 100.0) / len(cifar_train):.3f}%, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "    if (ep + 1) % 10 == 0:\n",
    "        calc_accuracy(student_model)\n",
    "\n",
    "calc_accuracy(student_model)\n",
    "\n",
    "torch.save(student_model.state_dict(), './models/cifar_student.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzItakSaJl7N"
   },
   "source": [
    "# Training Student model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWS2MU0hJ5J2",
    "outputId": "724c13fe-58cd-4a3d-a0f6-9971086cff0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 30, 30]             896\n",
      "       BatchNorm2d-2           [-1, 32, 30, 30]              64\n",
      "              ReLU-3           [-1, 32, 30, 30]               0\n",
      "         MaxPool2d-4           [-1, 32, 15, 15]               0\n",
      "            Conv2d-5           [-1, 64, 13, 13]          18,496\n",
      "       BatchNorm2d-6           [-1, 64, 13, 13]             128\n",
      "              ReLU-7           [-1, 64, 13, 13]               0\n",
      "         MaxPool2d-8             [-1, 64, 6, 6]               0\n",
      "           Flatten-9                 [-1, 2304]               0\n",
      "           Linear-10                  [-1, 128]         295,040\n",
      "             ReLU-11                  [-1, 128]               0\n",
      "          Dropout-12                  [-1, 128]               0\n",
      "           Linear-13                   [-1, 64]           8,256\n",
      "             ReLU-14                   [-1, 64]               0\n",
      "          Dropout-15                   [-1, 64]               0\n",
      "           Linear-16                   [-1, 10]             650\n",
      "          Softmax-17                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 323,530\n",
      "Trainable params: 323,530\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 1.23\n",
      "Estimated Total Size (MB): 2.25\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create a new model with the last layer removed, provides access to model logits\n",
    "student_model_wo_temperature = torch.nn.Sequential(\n",
    "    *(list(student_model.children())[:-1]),\n",
    "    nn.Softmax(dim=1)\n",
    ").to(device)\n",
    "print(summary(student_model_wo_temperature, (3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60mXuRlLDFSi",
    "outputId": "3773d6c4-8a2c-4030-cb36-79dd5a6528c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 2.09e+00, train acc: 46.627%, time: 23.87s\n",
      "epoch: 2, loss: 1.97e+00, train acc: 61.412%, time: 23.57s\n",
      "epoch: 3, loss: 1.93e+00, train acc: 66.478%, time: 23.73s\n",
      "epoch: 4, loss: 1.90e+00, train acc: 70.132%, time: 23.86s\n",
      "epoch: 5, loss: 1.89e+00, train acc: 71.497%, time: 23.58s\n",
      "epoch: 6, loss: 1.87e+00, train acc: 73.070%, time: 23.59s\n",
      "epoch: 7, loss: 1.86e+00, train acc: 74.332%, time: 23.82s\n",
      "epoch: 8, loss: 1.85e+00, train acc: 75.885%, time: 23.79s\n",
      "epoch: 9, loss: 1.85e+00, train acc: 76.777%, time: 23.73s\n",
      "epoch: 10, loss: 1.83e+00, train acc: 78.232%, time: 23.87s\n",
      "test accuracy: 65.430%\n",
      "epoch: 11, loss: 1.81e+00, train acc: 80.832%, time: 23.79s\n",
      "epoch: 12, loss: 1.81e+00, train acc: 81.892%, time: 23.79s\n",
      "epoch: 13, loss: 1.80e+00, train acc: 82.397%, time: 23.76s\n",
      "epoch: 14, loss: 1.80e+00, train acc: 82.780%, time: 23.78s\n",
      "epoch: 15, loss: 1.79e+00, train acc: 83.277%, time: 23.65s\n",
      "epoch: 16, loss: 1.79e+00, train acc: 83.517%, time: 23.57s\n",
      "epoch: 17, loss: 1.79e+00, train acc: 84.217%, time: 23.81s\n",
      "epoch: 18, loss: 1.79e+00, train acc: 83.820%, time: 23.68s\n",
      "epoch: 19, loss: 1.78e+00, train acc: 84.662%, time: 23.75s\n",
      "epoch: 20, loss: 1.78e+00, train acc: 84.862%, time: 23.83s\n",
      "test accuracy: 70.210%\n",
      "epoch: 21, loss: 1.78e+00, train acc: 84.980%, time: 23.70s\n",
      "epoch: 22, loss: 1.77e+00, train acc: 86.230%, time: 23.68s\n",
      "epoch: 23, loss: 1.77e+00, train acc: 86.005%, time: 23.70s\n",
      "epoch: 24, loss: 1.77e+00, train acc: 86.362%, time: 23.85s\n",
      "epoch: 25, loss: 1.77e+00, train acc: 86.312%, time: 23.82s\n",
      "epoch: 26, loss: 1.76e+00, train acc: 87.710%, time: 23.82s\n",
      "epoch: 27, loss: 1.76e+00, train acc: 88.077%, time: 23.68s\n",
      "epoch: 28, loss: 1.75e+00, train acc: 88.338%, time: 23.66s\n",
      "epoch: 29, loss: 1.75e+00, train acc: 88.592%, time: 23.80s\n",
      "epoch: 30, loss: 1.75e+00, train acc: 88.627%, time: 23.73s\n",
      "test accuracy: 73.720%\n",
      "test accuracy: 73.850%\n"
     ]
    }
   ],
   "source": [
    "# Training student from scratch using whole dataset\n",
    "student_model_wo_temperature.apply(weight_reset)\n",
    "student_optimizer = torch.optim.Adam(student_model_wo_temperature.parameters(), lr=0.001)\n",
    "student_loss_fn = nn.CrossEntropyLoss()\n",
    "student_lr_sch = torch.optim.lr_scheduler.MultiStepLR(student_optimizer, [10, 25, 40], gamma=0.5)\n",
    "\n",
    "kd_epochs = 30\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(kd_epochs):\n",
    "    ep_loss = 0.0\n",
    "    correct = 0\n",
    "    start_time = time_s()\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        imgs, true_labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        student_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass of the student\n",
    "        student_output = student_model_wo_temperature(imgs).to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        student_loss = student_loss_fn(student_output, true_labels)\n",
    "\n",
    "        student_loss.backward()\n",
    "        student_optimizer.step()\n",
    "\n",
    "        pred_class = torch.argmax(student_output, dim=1).to(device)\n",
    "        correct += len(true_labels) - torch.count_nonzero(pred_class - true_labels)\n",
    "        \n",
    "        ep_loss += student_loss.detach().item()\n",
    "\n",
    "    student_lr_sch.step()\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(train_dataset):.2e}, train acc: {(correct * 100.0) / len(cifar_train):.3f}%, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "    if (ep + 1) % 10 == 0:\n",
    "        calc_accuracy(student_model_wo_temperature)\n",
    "\n",
    "calc_accuracy(student_model_wo_temperature)\n",
    "\n",
    "torch.save(student_model_wo_temperature.state_dict(), './models/cifar_student_scratch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5-GVreSyBKk"
   },
   "source": [
    "# KD with ~3% of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkHRF_HPyHhz"
   },
   "outputs": [],
   "source": [
    "small_cifar_train_subset = list(range(0, int(len(cifar_train)*0.03)))\n",
    "small_cifar_train_dataset = torch.utils.data.DataLoader(torch.utils.data.Subset(cifar_train, small_cifar_train_subset), batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoZAG27fyrOK",
    "outputId": "5eb16e6b-2567-42b6-8433-3f6b884f2edb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.00e-01, train acc: 22.667%, time: 1.75s\n",
      "epoch: 2, loss: 9.63e-02, train acc: 27.333%, time: 1.67s\n",
      "epoch: 3, loss: 9.14e-02, train acc: 34.000%, time: 1.65s\n",
      "epoch: 4, loss: 8.72e-02, train acc: 37.533%, time: 1.68s\n",
      "epoch: 5, loss: 8.42e-02, train acc: 39.733%, time: 1.65s\n",
      "epoch: 6, loss: 8.42e-02, train acc: 40.267%, time: 1.62s\n",
      "epoch: 7, loss: 8.20e-02, train acc: 41.733%, time: 1.68s\n",
      "epoch: 8, loss: 7.88e-02, train acc: 43.267%, time: 1.66s\n",
      "epoch: 9, loss: 7.68e-02, train acc: 45.267%, time: 1.62s\n",
      "epoch: 10, loss: 7.56e-02, train acc: 45.600%, time: 1.68s\n",
      "test accuracy: 45.550%\n",
      "epoch: 11, loss: 6.94e-02, train acc: 47.067%, time: 1.72s\n",
      "epoch: 12, loss: 6.40e-02, train acc: 51.333%, time: 1.65s\n",
      "epoch: 13, loss: 6.15e-02, train acc: 52.333%, time: 1.66s\n",
      "epoch: 14, loss: 6.21e-02, train acc: 52.133%, time: 1.63s\n",
      "epoch: 15, loss: 5.90e-02, train acc: 54.933%, time: 1.65s\n",
      "epoch: 16, loss: 5.86e-02, train acc: 52.533%, time: 1.64s\n",
      "epoch: 17, loss: 5.72e-02, train acc: 54.733%, time: 1.64s\n",
      "epoch: 18, loss: 5.63e-02, train acc: 55.267%, time: 1.66s\n",
      "epoch: 19, loss: 5.39e-02, train acc: 56.200%, time: 1.65s\n",
      "epoch: 20, loss: 5.65e-02, train acc: 55.933%, time: 1.62s\n",
      "test accuracy: 51.080%\n",
      "epoch: 21, loss: 5.52e-02, train acc: 55.933%, time: 1.70s\n",
      "epoch: 22, loss: 5.30e-02, train acc: 56.000%, time: 1.64s\n",
      "epoch: 23, loss: 5.41e-02, train acc: 57.333%, time: 1.66s\n",
      "epoch: 24, loss: 5.02e-02, train acc: 57.267%, time: 1.65s\n",
      "epoch: 25, loss: 5.04e-02, train acc: 57.467%, time: 1.63s\n",
      "epoch: 26, loss: 4.66e-02, train acc: 60.933%, time: 1.65s\n",
      "epoch: 27, loss: 5.13e-02, train acc: 57.733%, time: 1.67s\n",
      "epoch: 28, loss: 4.58e-02, train acc: 60.667%, time: 1.64s\n",
      "epoch: 29, loss: 4.55e-02, train acc: 59.400%, time: 1.64s\n",
      "epoch: 30, loss: 4.68e-02, train acc: 59.467%, time: 1.64s\n",
      "test accuracy: 54.360%\n",
      "test accuracy: 54.020%\n"
     ]
    }
   ],
   "source": [
    "# KD from teacher using ~3% of dataset\n",
    "student_model.apply(weight_reset)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=0.003)\n",
    "student_lr_sch = torch.optim.lr_scheduler.MultiStepLR(student_optimizer, [10, 25, 40], gamma=0.3)\n",
    "student_loss_fn = nn.CrossEntropyLoss()\n",
    "distillation_loss_fn = torch.nn.KLDivLoss(reduction='mean')\n",
    "\n",
    "kd_epochs = 30\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(kd_epochs):\n",
    "    ep_loss = 0.0\n",
    "    correct = 0\n",
    "    start_time = time_s()\n",
    "    for i, (imgs, labels) in enumerate(small_cifar_train_dataset):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        student_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass of the teacher with input\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model_w_temperature(imgs).to(device)\n",
    "\n",
    "        # Forward pass of the student\n",
    "        student_output = student_model(imgs).to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        student_loss = student_loss_fn(student_output, labels)\n",
    "        distill_loss = distillation_loss_fn(teacher_output, student_output)\n",
    "        loss = (alpha * student_loss + (1 - alpha) * distill_loss) * temperature * temperature\n",
    "\n",
    "        loss.backward()\n",
    "        student_optimizer.step()\n",
    "\n",
    "        pred_class = torch.argmax(student_output, dim=1).to(device)\n",
    "        correct += len(labels) - torch.count_nonzero(pred_class - labels)\n",
    "\n",
    "        ep_loss += loss.detach().item()\n",
    "    student_lr_sch.step()\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(train_dataset):.2e}, train acc: {(correct * 100.0) / len(small_cifar_train_subset):.3f}%, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "    if (ep + 1) % 10 == 0:\n",
    "        calc_accuracy(student_model)\n",
    "\n",
    "calc_accuracy(student_model)\n",
    "\n",
    "torch.save(student_model.state_dict(), './models/small_cifar_student.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8A7bpiW29eD",
    "outputId": "4d324e9c-5fdc-4c92-a5d2-759556fe504f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.08e+00, train acc: 19.067%, time: 1.30s\n",
      "epoch: 2, loss: 1.06e+00, train acc: 23.000%, time: 1.28s\n",
      "epoch: 3, loss: 1.05e+00, train acc: 25.067%, time: 1.28s\n",
      "epoch: 4, loss: 1.05e+00, train acc: 26.267%, time: 1.27s\n",
      "epoch: 5, loss: 1.03e+00, train acc: 31.467%, time: 1.28s\n",
      "epoch: 6, loss: 1.03e+00, train acc: 30.067%, time: 1.34s\n",
      "epoch: 7, loss: 1.04e+00, train acc: 29.000%, time: 1.27s\n",
      "epoch: 8, loss: 1.04e+00, train acc: 29.467%, time: 1.31s\n",
      "epoch: 9, loss: 1.03e+00, train acc: 30.067%, time: 1.30s\n",
      "epoch: 10, loss: 1.03e+00, train acc: 31.400%, time: 1.30s\n",
      "test accuracy: 33.560%\n",
      "epoch: 11, loss: 1.01e+00, train acc: 35.400%, time: 1.32s\n",
      "epoch: 12, loss: 1.01e+00, train acc: 35.733%, time: 1.34s\n",
      "epoch: 13, loss: 1.00e+00, train acc: 36.000%, time: 1.30s\n",
      "epoch: 14, loss: 1.00e+00, train acc: 36.533%, time: 1.29s\n",
      "epoch: 15, loss: 9.93e-01, train acc: 38.533%, time: 1.28s\n",
      "epoch: 16, loss: 9.92e-01, train acc: 39.000%, time: 1.30s\n",
      "epoch: 17, loss: 9.83e-01, train acc: 40.867%, time: 1.29s\n",
      "epoch: 18, loss: 9.82e-01, train acc: 41.067%, time: 1.29s\n",
      "epoch: 19, loss: 9.81e-01, train acc: 40.933%, time: 1.29s\n",
      "epoch: 20, loss: 9.87e-01, train acc: 40.200%, time: 1.31s\n",
      "test accuracy: 39.790%\n",
      "epoch: 21, loss: 9.79e-01, train acc: 41.467%, time: 1.28s\n",
      "epoch: 22, loss: 9.76e-01, train acc: 42.400%, time: 1.29s\n",
      "epoch: 23, loss: 9.80e-01, train acc: 41.867%, time: 1.31s\n",
      "epoch: 24, loss: 9.78e-01, train acc: 41.400%, time: 1.30s\n",
      "epoch: 25, loss: 9.73e-01, train acc: 42.533%, time: 1.31s\n",
      "epoch: 26, loss: 9.76e-01, train acc: 41.733%, time: 1.32s\n",
      "epoch: 27, loss: 9.65e-01, train acc: 44.733%, time: 1.33s\n",
      "epoch: 28, loss: 9.71e-01, train acc: 42.867%, time: 1.29s\n",
      "epoch: 29, loss: 9.59e-01, train acc: 45.867%, time: 1.29s\n",
      "epoch: 30, loss: 9.64e-01, train acc: 44.600%, time: 1.28s\n",
      "test accuracy: 41.680%\n",
      "test accuracy: 42.340%\n"
     ]
    }
   ],
   "source": [
    "# Training student from scratch using small dataset\n",
    "student_model_wo_temperature.apply(weight_reset)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=0.003)\n",
    "student_lr_sch = torch.optim.lr_scheduler.MultiStepLR(student_optimizer, [10, 25, 40], gamma=0.3)\n",
    "student_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "kd_epochs = 30\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(kd_epochs):\n",
    "    ep_loss = 0.0\n",
    "    correct = 0\n",
    "    start_time = time_s()\n",
    "    for i, data in enumerate(small_cifar_train_dataset):\n",
    "        imgs, true_labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        student_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass of the student\n",
    "        student_output = student_model_wo_temperature(imgs).to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        student_loss = student_loss_fn(student_output, true_labels)\n",
    "\n",
    "        student_loss.backward()\n",
    "        student_optimizer.step()\n",
    "\n",
    "        pred_class = torch.argmax(student_output, dim=1).to(device)\n",
    "        correct += len(true_labels) - torch.count_nonzero(pred_class - true_labels)\n",
    "        \n",
    "        ep_loss += student_loss.detach().item()\n",
    "\n",
    "    student_lr_sch.step()\n",
    "    print(f'epoch: {ep+1}, loss: {ep_loss / len(train_dataset):.2e}, train acc: {(correct * 100.0) / len(small_cifar_train_subset):.3f}%, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "    if (ep + 1) % 10 == 0:\n",
    "        calc_accuracy(student_model_wo_temperature)\n",
    "\n",
    "calc_accuracy(student_model_wo_temperature)\n",
    "\n",
    "torch.save(student_model_wo_temperature.state_dict(), './models/small_cifar_student_scratch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCILoz9qp2lv"
   },
   "source": [
    "# Linearly increase Î± in KD from teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJ1Sco2PqL2x",
    "outputId": "9abf0e22-77ed-4958-ce8b-3d490fe9a23a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, alpha: 2.000e-01, loss: 4.12e+00, train acc: 41.376%, time: 26.10s\n",
      "epoch: 2, alpha: 2.000e-01, loss: 3.97e+00, train acc: 54.336%, time: 26.15s\n",
      "epoch: 3, alpha: 2.000e-01, loss: 3.91e+00, train acc: 58.612%, time: 26.07s\n",
      "epoch: 4, alpha: 2.000e-01, loss: 3.88e+00, train acc: 61.132%, time: 26.00s\n",
      "epoch: 5, alpha: 2.000e-01, loss: 3.85e+00, train acc: 62.912%, time: 26.09s\n",
      "epoch: 6, alpha: 2.000e-01, loss: 3.83e+00, train acc: 64.582%, time: 26.03s\n",
      "epoch: 7, alpha: 2.000e-01, loss: 3.81e+00, train acc: 65.692%, time: 26.23s\n",
      "epoch: 8, alpha: 2.000e-01, loss: 3.80e+00, train acc: 66.420%, time: 26.03s\n",
      "epoch: 9, alpha: 2.000e-01, loss: 3.79e+00, train acc: 67.308%, time: 25.82s\n",
      "epoch: 10, alpha: 2.000e-01, loss: 3.78e+00, train acc: 67.666%, time: 25.88s\n",
      "test accuracy: 71.630%\n",
      "epoch: 11, alpha: 2.000e-01, loss: 3.75e+00, train acc: 69.904%, time: 26.09s\n",
      "epoch: 12, alpha: 2.000e-01, loss: 3.75e+00, train acc: 69.962%, time: 26.28s\n",
      "epoch: 13, alpha: 2.000e-01, loss: 3.74e+00, train acc: 70.540%, time: 25.94s\n",
      "epoch: 14, alpha: 2.000e-01, loss: 3.74e+00, train acc: 70.818%, time: 25.70s\n",
      "epoch: 15, alpha: 2.000e-01, loss: 3.73e+00, train acc: 71.032%, time: 26.01s\n",
      "epoch: 16, alpha: 2.000e-01, loss: 3.72e+00, train acc: 71.544%, time: 26.06s\n",
      "epoch: 17, alpha: 2.000e-01, loss: 3.72e+00, train acc: 71.632%, time: 26.09s\n",
      "epoch: 18, alpha: 2.000e-01, loss: 3.72e+00, train acc: 71.904%, time: 26.04s\n",
      "epoch: 19, alpha: 2.000e-01, loss: 3.72e+00, train acc: 71.870%, time: 25.90s\n",
      "epoch: 20, alpha: 2.000e-01, loss: 3.71e+00, train acc: 72.474%, time: 25.77s\n",
      "test accuracy: 74.770%\n",
      "epoch: 21, alpha: 2.000e-01, loss: 3.71e+00, train acc: 72.606%, time: 26.09s\n",
      "epoch: 22, alpha: 2.000e-01, loss: 3.71e+00, train acc: 72.596%, time: 26.06s\n",
      "epoch: 23, alpha: 2.000e-01, loss: 3.71e+00, train acc: 72.686%, time: 25.88s\n",
      "epoch: 24, alpha: 2.000e-01, loss: 3.70e+00, train acc: 73.276%, time: 26.20s\n",
      "epoch: 25, alpha: 2.000e-01, loss: 3.70e+00, train acc: 73.124%, time: 25.98s\n",
      "epoch: 26, alpha: 2.000e-01, loss: 3.69e+00, train acc: 73.998%, time: 26.01s\n",
      "epoch: 27, alpha: 2.000e-01, loss: 3.69e+00, train acc: 74.088%, time: 26.28s\n",
      "epoch: 28, alpha: 2.000e-01, loss: 3.68e+00, train acc: 74.268%, time: 26.02s\n",
      "epoch: 29, alpha: 2.000e-01, loss: 3.68e+00, train acc: 74.376%, time: 25.82s\n",
      "epoch: 30, alpha: 2.000e-01, loss: 3.68e+00, train acc: 74.588%, time: 26.25s\n",
      "test accuracy: 76.910%\n",
      "test accuracy: 77.440%\n"
     ]
    }
   ],
   "source": [
    "# KD from teacher using whole dataset\n",
    "alpha = 0.1\n",
    "student_model.apply(weight_reset)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)\n",
    "student_lr_sch = torch.optim.lr_scheduler.MultiStepLR(student_optimizer, [10, 25, 40], gamma=0.5)\n",
    "student_loss_fn = nn.CrossEntropyLoss()\n",
    "distillation_loss_fn = torch.nn.KLDivLoss(reduction='mean')\n",
    "\n",
    "kd_epochs = 30\n",
    "time_s = lambda: time.time()\n",
    "for ep in range(kd_epochs):\n",
    "    ep_loss = 0.0\n",
    "    correct = 0\n",
    "    start_time = time_s()\n",
    "    alpha = max(0.1, 0.5 * (ep / kd_epochs))\n",
    "    for i, (imgs, labels) in enumerate(train_dataset):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        student_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass of the teacher with input\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model_w_temperature(imgs).to(device)\n",
    "\n",
    "        # Forward pass of the student\n",
    "        student_output = student_model(imgs).to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        student_loss = student_loss_fn(student_output, labels)\n",
    "        distill_loss = distillation_loss_fn(teacher_output, student_output)\n",
    "        loss = (alpha * student_loss + (1 - alpha) * distill_loss) * temperature * temperature\n",
    "\n",
    "        loss.backward()\n",
    "        student_optimizer.step()\n",
    "\n",
    "        pred_class = torch.argmax(student_output, dim=1).to(device)\n",
    "        correct += len(labels) - torch.count_nonzero(pred_class - labels)\n",
    "\n",
    "        ep_loss += loss.detach().item()\n",
    "\n",
    "    student_lr_sch.step()\n",
    "    print(f'epoch: {ep+1}, alpha: {alpha:.3e}, loss: {ep_loss / len(train_dataset):.2e}, train acc: {(correct * 100.0) / len(cifar_train):.3f}%, time: {(time_s() - start_time):.2f}s')\n",
    "\n",
    "    if (ep + 1) % 10 == 0:\n",
    "        calc_accuracy(student_model)\n",
    "\n",
    "calc_accuracy(student_model)\n",
    "\n",
    "torch.save(student_model.state_dict(), './models/cifar_student_linear_alpha.pt')\n",
    "\n",
    "# Looks like a schedule for increasing alpha doesn't have that much of an effect\n",
    "# By choosing alpha carefully you can still get equivalent results to schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCtjuWBi6fPA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QjNEs71Ju9aN",
    "fzItakSaJl7N",
    "z5-GVreSyBKk",
    "jCILoz9qp2lv"
   ],
   "name": "kd-cifar.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
